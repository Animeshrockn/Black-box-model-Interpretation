# Blackbox-model-Interpretation-using-LIME
Interpreting Black box model with K Medoids approach and comparison with LIME

In the era where technology is evolving and getting better day by day, we need to start questioning what we should choose to trust and what we shouldn’t. Machine learning has become the topic of interest as a technical aspect in the industry. Although it seems appealing to let the machine handle the human world problems but how does it achieve its goals? Machine learning comprises of models that are complex in its working. These black box models are widely used but its working is still a matter of interest to researchers. There have been various algorithms that try to mimic the working of the black box models and make it interpretable to humans. LIME is one such algorithm that works in local areas of the data set and extracts the importance of features that are interlinked with the final output. In this paper we explore LIME and try to find another method to interpret black box models not only for a single data point but in the global aspect as well. 

### Introduction 

Machine learning is a vast field that is associated with solving real work problems with the help of datasets. Initially, machine learning had simple models like naïve bayes and linear regression, which were mathematically interpretable. With time and exploration, machine learning became a complex field of study with the evolution of machine learning models. Machine learning was then divided into interpretable models and non-interpretable models (Black-box models).  Black box model is a system whose complexity makes it difficult to understand the inward working of the model, for example, neural networks and gradient boost models. The Figure-1 below reflects the working of a black box model, where inputs and outputs are visible to the user but the inside working is not understandable. Such models made it difficult to understand how the it concluded with the predictions. Although the accuracy, precision and recall are useful measures to judge a model but it is not enough to explain the predictions. Let us suppose, we have a dataset which predicts the sleeping hours of an individual. Now we are able to fit a model with 98 percent accuracy but the question still remains that “How did it predict 7 hours of sleep for a person X?” “What factors affected the output?” or perhaps “Which factors played a major role in deciding the sleeping hours?”. These questions are important when we want to know if the model is working as, it should or not. Blindly trusting a machine is not a responsible approach in solving a problem. What if the model is answering right because of some wrong features? Interpreting the model gives scope for improvement. For example, if we want to increase the sleeping hours of an individual and we know the factors that affected the person, we could easily find out how to increase the sleeping hours by working on those factors itself. 


LIME, Local Interpretable Model-Agnostic, was first coined in “Why Should I Trust You?” Explaining the Predictions of Any Classifier (KDD2016) by Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin [1]. The formal definition of LIME as provided by the paper is “It is a novel explanation technique that explains the prediction of a ML model in an interpretable and faithful manner by learning an interpretable model locally around the prediction” [1]. It became increasingly popular since it was able to interpret any black box model easily. It gave importance to the local aspect of the dataset to explain the global features. Our method is inspired by LIME to locally administer global black box model and explain its features, thereby, converting the complex model into a more interpretable model. 

